import streamlit as st
import os
import joblib
import numpy as np
import pandas as pd
import requests
import json
import pickle
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- è‡ªå®šä¹‰é¢„å¤„ç†ç±»å®šä¹‰ (from notebooks/preprocessing/02_preprocess.py) ---

class FeatureGeneratorWithNames(BaseEstimator, TransformerMixin):
    """æ”¯æŒç‰¹å¾åè¿½è¸ªçš„ç‰¹å¾ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.input_features_ = []
        self.output_features_ = []
        self.generated_features_ = []

    def fit(self, X: pd.DataFrame, y=None):
        self.input_features_ = list(X.columns)
        return self

    def transform(self, X: pd.DataFrame):
        X_ = X.copy()
        initial_features = set(X_.columns)
        
        # 1. Shock Index (ä¾èµ–: sbp, pulse)
        if all(c in X_.columns for c in ['sbp', 'pulse']):
            X_['shock_index'] = np.where(
                (X_['sbp'] > 0) & (X_['pulse'] > 0), 
                X_['pulse'] / X_['sbp'], 
                np.nan
            )
            X_['shock_flag'] = ((X_['shock_index'] > 0.9) & (X_['shock_index'].notna())).astype(int)

        # 2. SBP category (ä¾èµ–: sbp)
        if 'sbp' in X_.columns:
            X_['sbp_cat'] = pd.cut(
                X_['sbp'],
                bins=[0, 90, 120, 140, 200, np.inf],
                labels=['hypo','normal','elev','htn1','htn2']
            ).astype(str)

        # 3. Temperature category (ä¾èµ–: tempreture)
        if 'tempreture' in X_.columns:
            X_['temp_cat'] = pd.cut(
                X_['tempreture'],
                bins=[0, 35, 36.5, 37.5, 39, np.inf],
                labels=['hypo','normal','low_fever','fever','hyper']
            ).astype(str)

        # 4. Respiratory & Cardio flag (ä¾èµ–: res, pulse)
        if all(c in X_.columns for c in ['res', 'pulse']):
            X_['resp_cardio_flag'] = (
                (X_['res'] > 30) & (X_['pulse'] > 120) & 
                (X_['res'].notna()) & (X_['pulse'].notna())
            ).astype(int)

        # 5. Vital abnormal count (ä¾èµ–: sbp, tempreture, pulse, res)
        vital_cols = ['sbp', 'tempreture', 'pulse', 'res']
        if all(c in X_.columns for c in vital_cols):
            conditions = [
                ((X_['sbp'] < 90) | (X_['sbp'] > 180)) & (X_['sbp'].notna()),
                ((X_['tempreture'] < 36) | (X_['tempreture'] > 38.5)) & (X_['tempreture'].notna()),
                ((X_['pulse'] > 120) | (X_['pulse'] < 50)) & (X_['pulse'].notna()),
                ((X_['res'] > 30) | (X_['res'] < 10)) & (X_['res'].notna())
            ]
            X_['vital_abn_cnt'] = sum(cond.astype(int) for cond in conditions)

        # 6. MEWS high flag (ä¾èµ–: mews_total)
        if 'mews_total' in X_.columns:
            X_['mews_high'] = ((X_['mews_total'] >= 5) & (X_['mews_total'].notna())).astype(int)

        # 7. äº¤äº’é¡¹: age * shock_index (ä¾èµ–: age, shock_index)
        if all(c in X_.columns for c in ['age', 'shock_index']):
            X_['age_shock'] = X_['age'] * X_['shock_index'].fillna(0)

        # 8. äº¤äº’é¡¹: age * bmi (ä¾èµ–: age, bmi)
        if all(c in X_.columns for c in ['age', 'bmi']):
            X_['age_bmi'] = X_['age'] * X_['bmi'].fillna(0)

        # 9. Critical value features
        crit_cols = ['exam_critical_flag', 'lab_critical_flag']
        if all(c in X_.columns for c in crit_cols):
            X_['any_critical_flag'] = (
                (X_['exam_critical_flag'] == 1) | (X_['lab_critical_flag'] == 1)
            ).astype(int)
            X_['critical_count'] = X_['exam_critical_flag'] + X_['lab_critical_flag']
            
            if 'age' in X_.columns and 'any_critical_flag' in X_.columns:
                X_['age_critical_interact'] = X_['age'] * X_['any_critical_flag']
            
            if 'mews_total' in X_.columns and 'any_critical_flag' in X_.columns:
                X_['mews_critical_interact'] = X_['mews_total'].fillna(0) * X_['any_critical_flag']

        # 10. åˆ†ç®±ç‰¹å¾
        if 'age' in X_.columns:
            X_['age_bucket'] = pd.cut(
                X_['age'], 
                bins=[0,40,60,120], 
                labels=['young','middle','old']
            ).astype(str)
        
        if 'res' in X_.columns:
            X_['res_bin'] = pd.cut(
                X_['res'], 
                bins=[0,12,20,30,60], 
                labels=['low','normal','tachy','extreme']
            ).astype(str)
        
        if 'bmi' in X_.columns:
            X_['bmi_level'] = pd.cut(
                X_['bmi'], 
                bins=[0,18.5,24,28,32,100],
                labels=['under','normal','over','obese','morbid']
            ).astype(str)

        # è¿½è¸ªç”Ÿæˆçš„ç‰¹å¾
        final_features = set(X_.columns)
        self.generated_features_ = list(final_features - initial_features)
        self.output_features_ = list(X_.columns)
        
        return X_
    
    def get_feature_names_out(self, input_features=None):
        return np.array(self.output_features_)


class NamedAdaptivePCA(BaseEstimator, TransformerMixin):
    """æ”¯æŒç‰¹å¾åçš„è‡ªé€‚åº”PCA"""
    
    def __init__(self, total_dims=80, variance_threshold=0.85, min_dims=3,
                 group_variance=None, group_max_dims=None, scale_features=True):
        self.total_dims = total_dims
        self.variance_threshold = variance_threshold
        self.group_variance = group_variance or {}
        self.group_max_dims = group_max_dims or {}
        self.min_dims = min_dims
        self.scale_features = scale_features
        self.dims_allocation_ = {}
        self.pcas_ = {}
        self.scalers_ = {}
        self.feature_names_out_ = []
        self.emb_col_mapping_ = {}
    
    def _dynamic_split_embeddings(self, X):
        """åŠ¨æ€åˆ†ç»„embedding"""
        if hasattr(X, 'columns'):
            columns = X.columns
            X_array = X.values
        else:
            if hasattr(self, 'emb_col_mapping_') and self.emb_col_mapping_:
                emb_groups = {}
                for prefix, col_indices in self.emb_col_mapping_.items():
                    start_idx = min(col_indices)
                    end_idx = max(col_indices) + 1
                    emb_groups[prefix] = X[:, start_idx:end_idx]
                return emb_groups
            else:
                return self._infer_embedding_groups(X)

        emb_groups = {}
        for prefix in ['diag', 'hist', 'exam', 'lab']:
            emb_cols = [i for i, col in enumerate(columns) if col.startswith(f'{prefix}_emb_')]
            if emb_cols:
                start_idx = min(emb_cols)
                end_idx = max(emb_cols) + 1
                emb_groups[prefix] = X_array[:, start_idx:end_idx]
                self.emb_col_mapping_[prefix] = emb_cols
        
        return emb_groups
    
    def _infer_embedding_groups(self, X):
        """ä»æ•°æ®ç»´åº¦æ¨æ–­embeddingåˆ†ç»„"""
        emb_groups = {}
        total_cols = X.shape[1]
        
        if total_cols >= 400:
            group_size = total_cols // 4
            emb_groups['diag'] = X[:, :group_size]
            emb_groups['hist'] = X[:, group_size:2*group_size]
            emb_groups['exam'] = X[:, 2*group_size:3*group_size]
            emb_groups['lab'] = X[:, 3*group_size:]
        else:
            emb_groups['combined'] = X
        
        return emb_groups
    
    def fit(self, X, y=None):
        emb_groups = self._dynamic_split_embeddings(X)
        
        if not emb_groups:
            self.feature_names_out_ = []
            return self
        
        for name, emb_data in emb_groups.items():
            if emb_data.shape[1] == 0 or emb_data.shape[0] <= 1:
                continue
            
            if self.scale_features:
                self.scalers_[name] = StandardScaler(with_mean=False)
                emb_data_scaled = self.scalers_[name].fit_transform(emb_data)
            else:
                emb_data_scaled = emb_data
            
            var_thr = self.group_variance.get(name, self.variance_threshold)
            # ç¡®ä¿max_componentsä¸è¶…è¿‡æ ·æœ¬æ•°-1å’Œç‰¹å¾æ•°
            max_components = min(emb_data_scaled.shape[1], emb_data_scaled.shape[0] - 1, 100)
            
            # å¦‚æœmax_components <= 0ï¼Œè·³è¿‡è¿™ä¸ªç»„
            if max_components <= 0:
                continue
            
            pca_temp = PCA(n_components=max_components)
            pca_temp.fit(emb_data_scaled)
            
            cumsum = np.cumsum(pca_temp.explained_variance_ratio_)
            needed_dims = np.argmax(cumsum >= var_thr) + 1
            
            max_cap = self.group_max_dims.get(name, max_components)
            dims_final = int(min(max_cap, max(self.min_dims, needed_dims)))
            
            self.dims_allocation_[name] = dims_final
        
        self._normalize_allocation()
        
        self.feature_names_out_ = []
        for name, emb_data in emb_groups.items():
            if name in self.dims_allocation_:
                if self.scale_features and name in self.scalers_:
                    emb_data_scaled = self.scalers_[name].transform(emb_data)
                else:
                    emb_data_scaled = emb_data
                
                n_components = self.dims_allocation_[name]
                self.pcas_[name] = PCA(n_components=n_components, random_state=42)
                self.pcas_[name].fit(emb_data_scaled)
                
                for i in range(n_components):
                    var_explained = self.pcas_[name].explained_variance_ratio_[i]
                    feature_name = f'{name}_pca_{i:02d}_var{var_explained:.3f}'
                    self.feature_names_out_.append(feature_name)
        
        return self
    
    def transform(self, X):
        emb_groups = self._dynamic_split_embeddings(X)
        
        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ˜¯å•æ ·æœ¬é¢„æµ‹ä¸”PCAæœªè®­ç»ƒï¼Œè¿”å›é™ç»´åçš„ç‰¹å¾
        if X.shape[0] == 1 and not self.pcas_:
            # å¯¹äºå•æ ·æœ¬é¢„æµ‹ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„ç‰¹å¾è¡¨ç¤º
            transformed_parts = []
            for name, emb_data in emb_groups.items():
                if emb_data.shape[1] > 0:
                    # ç®€å•é™ç»´ï¼šå–å‰Nä¸ªç‰¹å¾
                    target_dims = min(20, emb_data.shape[1])  # æ¯ç»„æœ€å¤š20ç»´
                    simplified = emb_data[:, :target_dims]
                    transformed_parts.append(simplified)
            
            if transformed_parts:
                result = np.hstack(transformed_parts)
                return result
            else:
                return np.empty((X.shape[0], 0))
        
        transformed_parts = []
        for name, emb_data in emb_groups.items():
            if name in self.pcas_:
                if self.scale_features and name in self.scalers_:
                    emb_data_scaled = self.scalers_[name].transform(emb_data)
                else:
                    emb_data_scaled = emb_data
                
                transformed = self.pcas_[name].transform(emb_data_scaled)
                transformed_parts.append(transformed)
        
        if transformed_parts:
            result = np.hstack(transformed_parts)
            return result
        else:
            return np.empty((X.shape[0], 0))
    
    def get_feature_names_out(self, input_features=None):
        return np.array(self.feature_names_out_)
    
    def _normalize_allocation(self):
        if not self.dims_allocation_:
            return
            
        total_needed = sum(self.dims_allocation_.values())
        
        if total_needed != self.total_dims:
            scale_factor = self.total_dims / total_needed
            for name in self.dims_allocation_:
                self.dims_allocation_[name] = max(
                    self.min_dims, 
                    int(self.dims_allocation_[name] * scale_factor)
                )


class NamedCombinedPreprocessor(BaseEstimator, TransformerMixin):
    """æ”¯æŒç‰¹å¾åçš„ç»„åˆé¢„å¤„ç†å™¨"""
    
    def __init__(self, emb_processor):
        self.emb_processor = emb_processor
        self.non_emb_processor = None
        self.emb_cols = []
        self.num_cols = []
        self.cat_cols = []
        self.feature_names_out_ = []
    
    def fit(self, X, y=None):
        self.emb_cols = [c for c in X.columns if c.startswith(('diag_emb_','hist_emb_','exam_emb_','lab_emb_'))]
        
        obj_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
        numeric_categorical = ['exam_critical_flag', 'lab_critical_flag', 'o2', 'mews_aware']
        
        all_cat_cols = list(set(obj_cols + numeric_categorical))
        self.cat_cols = [c for c in all_cat_cols if c in X.columns]
        self.cat_cols.sort()
        
        self.num_cols = [c for c in X.columns 
                        if c not in self.emb_cols + self.cat_cols]
        self.num_cols.sort()
        
        transformers = []
        
        if self.num_cols:
            transformers.append(('num', Pipeline([
                ('imp', SimpleImputer(strategy='median')),
                ('scaler', StandardScaler())
            ]), self.num_cols))
        
        if self.cat_cols:
            transformers.append(('cat', Pipeline([
                ('imp', SimpleImputer(strategy='most_frequent')),
                ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
            ]), self.cat_cols))
        
        self.non_emb_processor = ColumnTransformer(
            transformers=transformers,
            remainder='drop'
        )
        
        X_non_emb = X.drop(columns=self.emb_cols)
        X_emb = X[self.emb_cols]
        
        if not X_non_emb.empty:
            self.non_emb_processor.fit(X_non_emb, y)
        
        if not X_emb.empty:
            self.emb_processor.fit(X_emb, y)
        
        self._generate_feature_names()
        
        return self
    
    def _generate_feature_names(self):
        """ç”Ÿæˆå®Œæ•´çš„ç‰¹å¾ååˆ—è¡¨"""
        self.feature_names_out_ = []
        
        self.feature_names_out_.extend(self.num_cols)
        
        if self.cat_cols:
            cat_pipeline = self.non_emb_processor.named_transformers_['cat']
            if hasattr(cat_pipeline.named_steps['ohe'], 'get_feature_names_out'):
                cat_feature_names = cat_pipeline.named_steps['ohe'].get_feature_names_out(self.cat_cols)
                self.feature_names_out_.extend(cat_feature_names)
            else:
                for col in self.cat_cols:
                    self.feature_names_out_.append(f'{col}_encoded')
        
        if hasattr(self.emb_processor, 'feature_names_out_'):
            self.feature_names_out_.extend(self.emb_processor.feature_names_out_)
    
    def transform(self, X):
        X_non_emb = X.drop(columns=self.emb_cols)
        X_emb = X[self.emb_cols]
        
        transformed_parts = []
        
        if not X_non_emb.empty and hasattr(self, 'non_emb_processor'):
            X_non_emb_transformed = self.non_emb_processor.transform(X_non_emb)
            transformed_parts.append(X_non_emb_transformed)
        
        if not X_emb.empty:
            X_emb_transformed = self.emb_processor.transform(X_emb)
            if X_emb_transformed.shape[1] > 0:
                transformed_parts.append(X_emb_transformed)
        
        if transformed_parts:
            result = np.hstack(transformed_parts)
            return result
        else:
            return np.empty((X.shape[0], 0))
    
    def get_feature_names_out(self, input_features=None):
        return np.array(self.feature_names_out_)

# --- Page Configuration ---
st.set_page_config(page_title="ICU Risk Prediction - CatBoost SMOTETomek 0.05", layout="wide")

# --- Configuration ---
DEFAULT_SILICONFLOW_API_URL = "https://api.siliconflow.cn/v1/embeddings"
DEFAULT_SILICONFLOW_API_KEY = "YOUR_SILICONFLOW_API_KEY_HERE"
DEFAULT_BGE_MODEL_NAME = "BAAI/bge-m3"


# --- Model and File Paths ---
# ç›´æ¥ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œé€‚ç”¨äºStreamlit Cloudéƒ¨ç½²
MODEL_PATH = 'results/feature_optimized_training_20250705_222545/best_model.pkl'
PIPELINE_PATH = 'processed_data/preprocess_pipeline.pkl'
FEATURES_PATH = 'processed_data/final_features_list.txt'

# æœ€ä½³æ¨¡å‹ä¿¡æ¯ï¼ˆåŸºäºcatboost_smotetomek_0.05æ¨¡å‹ï¼‰
MODEL_INFO = {
    'name': 'CatBoost with SMOTETomek (0.05) - Feature Optimized Best Model',
    'combination': 'catboost_smotetomek_0.05',
    'model_type': 'CatBoost',
    'sampler': 'SMOTETomek',
    'sampling_ratio': 0.05,
    'selection_basis': 'Validation AUPRC (Best Performance)',
    'stage1_cv_auprc': 0.2685,  # ç¬¬ä¸€é˜¶æ®µCV AUPRC
    'stage2_cv_auprc': 0.2863,  # ç¬¬äºŒé˜¶æ®µCV AUPRC
    'valid_auprc': 0.3132,      # éªŒè¯é›†AUPRC (æœ€é«˜)
    'test_auprc': 0.3175,       # æµ‹è¯•é›†AUPRC
    'test_auc': 0.9230,         # æµ‹è¯•é›†AUC
    'test_recall': 0.3309,      # æµ‹è¯•é›†å¬å›ç‡
    'test_precision': 0.4894,   # æµ‹è¯•é›†ç²¾ç¡®ç‡
    'test_f1': 0.3948,          # æµ‹è¯•é›†F1
    'optimal_threshold': 0.2142, # æœ€ä¼˜é˜ˆå€¼
    'generalization_score': 1.109,  # æ³›åŒ–èƒ½åŠ›å¾—åˆ† (test_auprc/stage1_cv_auprc)
    'note': 'Best model after removing admission_unit feature - CatBoost excels at handling categorical features and imbalanced data'
}

# é˜ˆå€¼è®¾ç½®ï¼ˆåŸºäºæœ€ä½³æ¨¡å‹çš„é˜ˆå€¼ï¼‰
MODEL_THRESHOLD_OPTIMAL = MODEL_INFO['optimal_threshold']    # 0.2142 (æœ€ä¼˜é˜ˆå€¼)
MODEL_THRESHOLD_MEDICAL = 0.18     # åŒ»ç–—å¹³è¡¡é˜ˆå€¼ (æ›´ä¿å®ˆ)
MODEL_THRESHOLD_SENSITIVE = 0.03   # é«˜æ•æ„Ÿæ€§é˜ˆå€¼ (æ›´æ—©é¢„è­¦) - ä¼˜åŒ–åçš„æ•æ„Ÿæ€§ä¼˜å…ˆé˜ˆå€¼
MODEL_THRESHOLD_SPECIFIC = 0.25    # é«˜ç‰¹å¼‚æ€§é˜ˆå€¼ (å‡å°‘å‡é˜³æ€§)

# æ›´æ–°æ¨¡å‹ä¿¡æ¯çš„é˜ˆå€¼
MODEL_INFO.update({
    'threshold_optimal': MODEL_THRESHOLD_OPTIMAL,
    'threshold_medical': MODEL_THRESHOLD_MEDICAL,
    'threshold_sensitive': MODEL_THRESHOLD_SENSITIVE,
    'threshold_specific': MODEL_THRESHOLD_SPECIFIC
})

# --- èµ„æºåŠ è½½å‡½æ•° ---
@st.cache_resource
def load_resources():
    """Load model and preprocessing pipeline"""
    try:
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if os.path.exists(MODEL_PATH):
            model_dict = joblib.load(MODEL_PATH)
            # æ¨¡å‹æ–‡ä»¶æ˜¯å­—å…¸æ ¼å¼ï¼Œå®é™…æ¨¡å‹åœ¨pipelineé”®ä¸­
            if isinstance(model_dict, dict) and 'pipeline' in model_dict:
                model = model_dict['pipeline']
                st.success("âœ… Model loaded successfully")
            else:
                model = model_dict  # ç›´æ¥æ˜¯æ¨¡å‹å¯¹è±¡
                st.success("âœ… Model loaded successfully")
        else:
            st.error(f"âŒ Model file not found: {MODEL_PATH}")
            return None, None, None
        
        # å°è¯•åŠ è½½é¢„å¤„ç†pipelineï¼ˆç›´æ¥ä½¿ç”¨ï¼Œä¸é‡æ–°åˆ›å»ºï¼‰
        if os.path.exists(PIPELINE_PATH):
            try:
                # ç›´æ¥åŠ è½½é¢„è®­ç»ƒçš„pipeline
                preprocessing_pipeline = joblib.load(PIPELINE_PATH)
                st.success("âœ… Preprocessing pipeline loaded successfully")
                return model, preprocessing_pipeline, None
            except Exception as e:
                st.error(f"âŒ Failed to load preprocessing pipeline: {str(e)}")
                st.error("Cannot load pretrained pipeline, please check file integrity")
                return None, None, None
        else:
            st.error(f"âŒ Pipeline file not found: {PIPELINE_PATH}")
            return None, None, None
            
        return model, pipeline, feature_names
        
    except Exception as e:
        st.error(f"âŒ Resource loading failed: {str(e)}")
        return None, None, None

# --- Embedding API å‡½æ•° ---
def get_bge_embedding(text: str, api_key: str, api_url: str, model_name: str) -> list[float]:
    """Get BGE embedding"""
    if not text.strip():
        return [0.0] * 1024  # è¿”å›é›¶å‘é‡
    
    # æ£€æŸ¥APIé…ç½®
    if not api_key or api_key == "YOUR_SILICONFLOW_API_KEY_HERE":
        st.error("âŒ SiliconFlow API Key not configured! Please enter a valid API Key in the sidebar")
        return [0.0] * 1024
    
    if not api_url or not model_name:
        st.error("âŒ API URL or model name not configured!")
        return [0.0] * 1024
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": model_name,
        "input": [text],
        "encoding_format": "float"
    }
    
    try:
        response = requests.post(api_url, headers=headers, json=data, timeout=30)
        if response.status_code == 200:
            result = response.json()
            if 'data' in result and len(result['data']) > 0:
                return result['data'][0]['embedding']
        elif response.status_code == 401:
            st.error("âŒ API authentication failed! Please check if the API Key is correct")
        elif response.status_code == 429:
            st.error("âŒ API rate limit exceeded! Please try again later")
        else:
            st.error(f"âŒ API call failed! Status code: {response.status_code}")
        return [0.0] * 1024
    except requests.exceptions.Timeout:
        st.error("âŒ API call timeout! Please check network connection")
        return [0.0] * 1024
    except Exception as e:
        st.error(f"âŒ API call exception: {str(e)}")
        return [0.0] * 1024

# --- ç‰¹å¾å¤„ç†å‡½æ•° ---
def process_features_with_pipeline(data_input: dict, text_inputs: dict, sf_api_key: str, sf_api_url: str, sf_model_name: str, preprocessing_pipeline):
    """Process features using pretrained pipeline - fully matching training process"""
    try:
        st.info("ğŸ”§ Processing features...")
        
        # æ­¥éª¤1: åˆ›å»ºå®Œå…¨åŒ¹é…è®­ç»ƒæ•°æ®æ ¼å¼çš„DataFrame
        # åŸºç¡€ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
        base_features = {
            'age': data_input.get('age', 50),
            'height': data_input.get('height', 170),
            'weight': data_input.get('weight', 65),
            'bmi': data_input.get('bmi', 25), 
            'pulse': data_input.get('pulse', 80),
            'tempreture': data_input.get('tempreture', 37),
            'sbp': data_input.get('sbp', 120),
            'res': data_input.get('res', 16),
            'mews_total': data_input.get('mews_total', 1),
            'gender': data_input.get('gender', 0),
            'admission_unit': data_input.get('admission_unit', 0),
            'surgey': data_input.get('surgey', 0),
            'intervention': data_input.get('intervention', 0),
            'exam_critical_flag': data_input.get('exam_critical_flag', 0),
            'lab_critical_flag': data_input.get('lab_critical_flag', 0),
            'o2': data_input.get('o2', 0),
            'mews_aware': data_input.get('mews_aware', 0)
        }
        
        # æ­¥éª¤2: è·å–æ–‡æœ¬åµŒå…¥ï¼ˆ4ç»„ï¼Œæ¯ç»„1024ç»´ï¼‰
        st.info("ğŸ”„ Getting text embeddings...")
        
        text_mappings = {
            "admission_diagnosis": "diag",
            "history": "hist", 
            "exam_critical_value": "exam",
            "lab_critical_value": "lab"
        }
        
        embedding_features = {}
        zero_vector_count = 0
        
        for text_key, emb_prefix in text_mappings.items():
            try:
                text_content = text_inputs.get(text_key, "").strip()
                if not text_content or text_content.lower() in ['', 'none', 'n/a', 'na']:
                    # æä¾›æœ‰æ„ä¹‰çš„é»˜è®¤æ–‡æœ¬
                    default_texts = {
                        "admission_diagnosis": "General medical condition under observation",
                        "history": "No significant past medical history reported", 
                        "exam_critical_value": "No critical physical examination findings",
                        "lab_critical_value": "No critical laboratory abnormalities detected"
                    }
                    text_content = default_texts[text_key]
                
                # è·å–1024ç»´åµŒå…¥å‘é‡
                embedding = get_bge_embedding(text_content, sf_api_key, sf_api_url, sf_model_name)
                if embedding and len(embedding) >= 1024 and not all(x == 0.0 for x in embedding[:10]):  # æ£€æŸ¥å‰10ä¸ªå€¼æ˜¯å¦éƒ½ä¸º0
                    # ä½¿ç”¨ä¸è®­ç»ƒæ—¶å®Œå…¨ç›¸åŒçš„å‘½åæ ¼å¼
                    for i in range(1024):
                        embedding_features[f'{emb_prefix}_emb_{i}'] = embedding[i]
                    st.success(f"âœ… Successfully obtained {text_key} embedding: {len(embedding)} dims")
                else:
                    st.warning(f"âš ï¸ {text_key} embedding failed, using zero vector (will affect prediction accuracy)")
                    zero_vector_count += 1
                    for i in range(1024):
                        embedding_features[f'{emb_prefix}_emb_{i}'] = 0.0
                        
            except Exception as e:
                st.error(f"âŒ Processing {text_key} failed: {e}")
                zero_vector_count += 1
                for i in range(1024):
                    embedding_features[f'{emb_prefix}_emb_{i}'] = 0.0
        
        # æ˜¾ç¤ºembeddingçŠ¶æ€æ€»ç»“
        if zero_vector_count > 0:
            st.error(f"ğŸš¨ Warning: {zero_vector_count}/4 text embeddings used zero vector, this will significantly affect prediction accuracy!")
            st.error("Please configure correct SiliconFlow API Key to get accurate prediction results.")
        else:
            st.success("âœ… All text embeddings obtained successfully!")
        
        # æ­¥éª¤3: åˆå¹¶æ‰€æœ‰ç‰¹å¾ä¸ºå®Œæ•´çš„DataFrame
        all_features = {**base_features, **embedding_features}
        input_df = pd.DataFrame([all_features])
        
        # éªŒè¯ç‰¹å¾æ•°é‡
        expected_emb_cols = 4 * 1024  # 4 groups of embedding, each 1024 dims
        actual_emb_cols = len([col for col in input_df.columns if '_emb_' in col])
        st.info(f"ğŸ“Š Feature validation: {len(base_features)} base + {actual_emb_cols} embedding = {len(all_features)} total")
        
        # æ­¥éª¤4: åº”ç”¨é¢„å¤„ç†pipeline
        # Pipelineæ‰§è¡Œé¡ºåº: FeatureGenerator -> CombinedPreprocessor (non_emb + emb processing)
        st.info("ğŸ”„ Applying preprocessing pipeline...")
        processed_features = preprocessing_pipeline.transform(input_df)
        
        # ç¡®ä¿è¿”å›numpyæ•°ç»„ç”¨äºæ¨¡å‹é¢„æµ‹
        if hasattr(processed_features, 'toarray'):
            processed_features = processed_features.toarray()
        
        st.success(f"âœ… Preprocessing complete: {processed_features.shape}")
        return processed_features
        
    except Exception as e:
        st.error(f"âŒ Preprocessing pipeline error: {str(e)}")
        import traceback
        st.error(f"Traceback: {traceback.format_exc()}")
        return None

# --- Clinical Risk Assessment Function ---
def assess_clinical_risk(data_payload):
    """Clinical risk assessment rules"""
    risk_factors = []
    risk_score = 0
    
    # Basic vital signs risk
    if data_payload.get('sbp', 120) < 90:
        risk_factors.append("Hypotension (SBP < 90)")
        risk_score += 3
    elif data_payload.get('sbp', 120) > 180:
        risk_factors.append("Hypertensive crisis (SBP > 180)")
        risk_score += 2
    
    if data_payload.get('pulse', 80) > 120:
        risk_factors.append("Tachycardia (HR > 120)")
        risk_score += 2
    elif data_payload.get('pulse', 80) < 50:
        risk_factors.append("Bradycardia (HR < 50)")
        risk_score += 2
    
    if data_payload.get('tempreture', 37) > 38.5:
        risk_factors.append("Fever (Temp > 38.5Â°C)")
        risk_score += 1
    elif data_payload.get('tempreture', 37) < 36:
        risk_factors.append("Hypothermia (< 36Â°C)")
        risk_score += 2
    
    if data_payload.get('res', 16) > 30:
        risk_factors.append("Tachypnea (RR > 30)")
        risk_score += 2
    elif data_payload.get('res', 16) < 10:
        risk_factors.append("Respiratory depression (RR < 10)")
        risk_score += 3
    
    # Age risk
    age = data_payload.get('age', 50)
    if age > 80:
        risk_factors.append("Advanced age (> 80 years)")
        risk_score += 2
    elif age > 65:
        risk_factors.append("Elderly (> 65 years)")
        risk_score += 1
    
    # MEWS score risk
    mews = data_payload.get('mews_total', 0)
    if mews >= 5:
        risk_factors.append(f"High MEWS score ({mews} points)")
        risk_score += 3
    elif mews >= 3:
        risk_factors.append(f"Moderate MEWS score ({mews} points)")
        risk_score += 1
    
    # Surgery risk
    if data_payload.get('surgey', 0):
        risk_factors.append("Recent surgery")
        risk_score += 1
    
    # Calculate shock index
    shock_index = data_payload.get('pulse', 80) / max(data_payload.get('sbp', 120), 1)
    if shock_index > 0.9:
        risk_factors.append(f"Abnormal shock index ({shock_index:.2f})")
        risk_score += 3
    
    # Risk level
    if risk_score >= 8:
        risk_level = "ğŸ”´ Very High Risk"
        recommendation = "Immediate ICU assessment, continuous monitoring"
    elif risk_score >= 5:
        risk_level = "ğŸŸ¡ High Risk"
        recommendation = "Enhanced monitoring, consider ICU assessment"
    elif risk_score >= 3:
        risk_level = "ğŸŸ  Moderate Risk"
        recommendation = "Close observation, regular assessment"
    else:
        risk_level = "ğŸŸ¢ Low Risk"
        recommendation = "Routine monitoring"
    
    return {
        'risk_factors': risk_factors,
        'risk_score': risk_score,
        'risk_level': risk_level,
        'recommendation': recommendation
    }

# --- Prediction Function ---
def make_prediction(processed_features, data_payload=None, threshold_mode="optimal"):
    """Make prediction using the best model"""
    model, _, _ = load_resources()
    
    if model is None or processed_features is None:
        return None
    
    try:
        # è·å–é¢„æµ‹æ¦‚ç‡
        pred_proba = model.predict_proba(processed_features)[0, 1]
        
        # æ ¹æ®é˜ˆå€¼æ¨¡å¼é€‰æ‹©é˜ˆå€¼
        thresholds = {
            "optimal": MODEL_THRESHOLD_OPTIMAL,
            "medical": MODEL_THRESHOLD_MEDICAL, 
            "sensitive": MODEL_THRESHOLD_SENSITIVE,
            "specific": MODEL_THRESHOLD_SPECIFIC
        }
        
        threshold = thresholds.get(threshold_mode, MODEL_THRESHOLD_OPTIMAL)
        prediction = 1 if pred_proba >= threshold else 0
        
        # Risk level
        if pred_proba >= 0.3:
            risk_level = "ğŸ”´ Very High Risk"
        elif pred_proba >= 0.2:
            risk_level = "ğŸŸ¡ High Risk"
        elif pred_proba >= 0.1:
            risk_level = "ğŸŸ  Moderate Risk"
        else:
            risk_level = "ğŸŸ¢ Low Risk"
        
        # Clinical assessment
        clinical_assessment = None
        if data_payload:
            clinical_assessment = assess_clinical_risk(data_payload)
        
        return {
            'prediction': prediction,
            'probability': pred_proba,
            'threshold': threshold,
            'risk_level': risk_level,
            'clinical_assessment': clinical_assessment,
            'model_info': MODEL_INFO
        }
        
    except Exception as e:
        st.error(f"âŒ Prediction failed: {str(e)}")
        return None

# --- Main Interface ---
def main():
    st.title("ğŸ¥ ICU Risk Prediction System")
    st.subheader("Powered by CatBoost + SMOTETomek (0.03)")
    
    # ä¾§è¾¹æ  - ç®€åŒ–é…ç½®
    with st.sidebar:
        # é˜ˆå€¼è®¾ç½®
        with st.expander("ğŸšï¸ Threshold Settings"):
            threshold_mode = st.selectbox(
                "Select prediction threshold mode",
                ["sensitive", "optimal", "medical", "specific"],
                format_func=lambda x: {
                    "optimal": f"Optimal ({MODEL_THRESHOLD_OPTIMAL:.2f})",
                    "medical": f"Medical Balance ({MODEL_THRESHOLD_MEDICAL:.2f})",
                    "sensitive": f"High Sensitivity ({MODEL_THRESHOLD_SENSITIVE:.2f})",
                    "specific": f"High Specificity ({MODEL_THRESHOLD_SPECIFIC:.2f})"
                }[x]
            )
        
        # APIé…ç½®
        with st.expander("ğŸ”§ API Configuration (Required)", expanded=True):
            st.warning("âš ï¸ Text embedding requires SiliconFlow API configuration, otherwise prediction accuracy will be significantly reduced!")
            sf_api_key = st.text_input("SiliconFlow API Key", 
                                     value=DEFAULT_SILICONFLOW_API_KEY,
                                     type="password",
                                     help="Please enter a valid SiliconFlow API Key")
            sf_api_url = st.text_input("API URL", 
                                     value=DEFAULT_SILICONFLOW_API_URL)
            sf_model_name = st.text_input("Model Name", 
                                        value=DEFAULT_BGE_MODEL_NAME)
            
            # APIçŠ¶æ€æ£€æŸ¥
            if sf_api_key == DEFAULT_SILICONFLOW_API_KEY:
                st.error("ğŸš¨ API Key not configured! Zero vectors will be used instead of text embeddings")
            else:
                st.success("âœ… API Key configured")
    
    # ä¸»è¦è¾“å…¥åŒºåŸŸ
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.header("ğŸ“‹ Basic Information")
        
        # åŸºç¡€ä¿¡æ¯è¾“å…¥ - ç§»é™¤admission_unitä»¥é¿å…è¿‡æ‹Ÿåˆ
        # admission_unit = st.selectbox("å…¥é™¢ç§‘å®¤", [0, 1, 2], 
        #                             format_func=lambda x: ["ç§‘å®¤A", "ç§‘å®¤B", "ç§‘å®¤C"][x])
        admission_unit = 0  # è®¾ç½®é»˜è®¤å€¼ï¼Œä¸æ˜¾ç¤ºç»™ç”¨æˆ·
        age = st.number_input("Age", min_value=0, max_value=120, value=65)
        gender = st.selectbox("Gender", [0, 1], format_func=lambda x: ["Female", "Male"][x])
        height = st.number_input("Height (cm)", min_value=100.0, max_value=220.0, value=170.0)
        weight = st.number_input("Weight (kg)", min_value=30.0, max_value=200.0, value=70.0)
        
        # è®¡ç®—BMI
        bmi = weight / ((height/100) ** 2)
        st.info(f"BMI: {bmi:.1f}")
        
        # ç”Ÿç†æŒ‡æ ‡
        st.subheader("ğŸ«€ Vital Signs")
        sbp = st.number_input("Systolic BP (mmHg)", min_value=50, max_value=250, value=120)
        pulse = st.number_input("Heart Rate (bpm)", min_value=30, max_value=200, value=80)
        tempreture = st.number_input("Temperature (Â°C)", min_value=33.0, max_value=45.0, value=37.0, step=0.1)
        res = st.number_input("Respiratory Rate (/min)", min_value=5, max_value=60, value=16)
        
        # MEWSç›¸å…³
        st.subheader("ğŸ¥ Medical Assessment")
        mews_total = st.number_input("MEWS Total Score", min_value=0, max_value=20, value=2)
        mews_aware = st.selectbox("Consciousness Level", [0, 1, 2, 3], 
                                format_func=lambda x: ["Alert", "Drowsy", "Coma", "Other"][x])
        
        # å…¶ä»–ä¿¡æ¯
        surgey = st.checkbox("Recent Surgery")
        intervention = st.checkbox("Intervention Treatment")
        o2 = st.selectbox("Oxygen Therapy", [0, 1], format_func=lambda x: ["No", "Yes"][x])
    
    with col2:
        st.header("ğŸ“ Clinical Text")
        
        # æ–‡æœ¬è¾“å…¥
        admission_diagnosis = st.text_area("Admission Diagnosis", 
                                         placeholder="Enter admission diagnosis...",
                                         height=100)
        
        history = st.text_area("Medical History", 
                             placeholder="Enter medical history...",
                             height=100)
        
        exam_critical_value = st.text_area("Critical Examination Values", 
                                         placeholder="Enter critical examination values...",
                                         height=80)
        
        lab_critical_value = st.text_area("Critical Laboratory Values", 
                                        placeholder="Enter critical laboratory values...",
                                        height=80)
        
        # å¼‚å¸¸æ ‡è®°
        st.subheader("âš ï¸ Critical Flags")
        exam_critical_flag = st.checkbox("Examination Critical Flag")
        lab_critical_flag = st.checkbox("Laboratory Critical Flag")
    
    # é¢„æµ‹æŒ‰é’®
    if st.button("ğŸ”® Predict ICU Risk", type="primary", use_container_width=True):
        # å‡†å¤‡æ•°æ®
        data_input = {
            'admission_unit': admission_unit,
            'age': age,
            'gender': gender,
            'height': height,
            'weight': weight,
            'bmi': bmi,
            'sbp': sbp,
            'pulse': pulse,
            'tempreture': tempreture,
            'res': res,
            'mews_total': mews_total,
            'mews_aware': mews_aware,
            'surgey': int(surgey),
            'intervention': int(intervention),
            'o2': o2,
            'exam_critical_flag': int(exam_critical_flag),
            'lab_critical_flag': int(lab_critical_flag)
        }
        
        text_inputs = {
            'admission_diagnosis': admission_diagnosis,
            'history': history,
            'exam_critical_value': exam_critical_value,
            'lab_critical_value': lab_critical_value
        }
        
        # åŠ è½½èµ„æº
        model, pipeline, _ = load_resources()
        
        if model and pipeline:
            # å¤„ç†ç‰¹å¾
            processed_features = process_features_with_pipeline(
                data_input, text_inputs, 
                sf_api_key, sf_api_url, sf_model_name, 
                pipeline
            )
            
            if processed_features is not None:
                # è¿›è¡Œé¢„æµ‹
                result = make_prediction(processed_features, data_input, threshold_mode)
                
                if result:
                    # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
                    st.header("ğŸ¯ Prediction Results")
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric("ICU Admission Probability", f"{result['probability']:.1%}")
                    
                    with col2:
                        st.metric("Threshold Used", f"{result['threshold']:.2f}")
                    
                    with col3:
                        prediction_text = "ICU Required" if result['prediction'] else "No ICU Required"
                        st.metric("Prediction Result", prediction_text)
                    
                    # é£é™©ç­‰çº§
                    st.write(f"**Risk Level**: {result['risk_level']}")
                    
                    # ä¸´åºŠè¯„ä¼°
                    if result['clinical_assessment']:
                        clinical = result['clinical_assessment']
                        
                        st.subheader("ğŸ‘¨â€âš•ï¸ Clinical Risk Assessment")
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            st.write(f"**Risk Level**: {clinical['risk_level']}")
                            st.write(f"**Risk Score**: {clinical['risk_score']}")
                        
                        with col2:
                            st.write(f"**Recommendation**: {clinical['recommendation']}")
                        
                        if clinical['risk_factors']:
                            st.write("**Identified Risk Factors**:")
                            for factor in clinical['risk_factors']:
                                st.write(f"â€¢ {factor}")

if __name__ == "__main__":
    main()
